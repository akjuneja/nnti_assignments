{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb034177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "# 7.5 Build your own regularized NN\n",
    "\n",
    "In this exercise you get to use your previously built networks, but this time you need to add regularization in the form of dropout and $L_2$-regularization.\n",
    "\n",
    "Each layer has the option of using dropout. Your code needs to allow for this flexibility.\n",
    "\n",
    "Additionally, adding $L_2$-regularization should also be optional upon creation.\n",
    "\n",
    "**NOTE**: You are allowed to use built-in functions from pytorch to incorporate this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66f378",
   "metadata": {},
   "source": [
    "### 7.5.1 Implement a regularized model (1 point)\n",
    "\n",
    "Implement your own model (using `torch`) using the skeleton code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement a model that incorporates dropout and L2 regularization\n",
    "    depending on arguments passed.\n",
    "    \n",
    "    Args:\n",
    "    input_dim: dimensionality of the inputs\n",
    "    hidden_dim: how many units each hidden layer will have\n",
    "    out_dim: how many output units\n",
    "    num_layers: how many hidden layers to create/use\n",
    "    dropout: a list of booleans specifying which hidden layers will have dropout\n",
    "    dropout_p: the probability used for the `Dropout` layers\n",
    "    l2_reg: a boolean value that indicates whether L2 regularization should be used\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 out_dim: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: list,\n",
    "                 dropout_p: float,\n",
    "                 l2_reg: bool):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_list = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.l2_reg = l2_reg\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Define the layers of the network\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        # Use the dropout layer if specified\n",
    "        if self.dropout_p > 0.0:\n",
    "            self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        i=0\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = torch.relu(x)   \n",
    "            if self.dropout_p > 0.0 and self.dropout_list[i]:\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "            i+=1\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer, lambda_l2):\n",
    "\n",
    "    # Total size of dataset for reference\n",
    "    size = 0\n",
    "\n",
    "    # places your model into training mode\n",
    "    model.train()\n",
    "\n",
    "    correct = 0\n",
    "    _correct = 0\n",
    "    \n",
    "    y_true = torch.tensor(()).to(device)\n",
    "    y_pred = torch.tensor(()).to(device)\n",
    "\n",
    "    # Gives X , y for each batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Converting device to cuda\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        # Compute prediction error / loss\n",
    "        # 1. Compute y_pred \n",
    "        # 2. Compute loss between y and y_pred using selectd loss function\n",
    "\n",
    "        pred = model.forward(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        y_true = torch.cat((y_true, y), 0)\n",
    "        y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
    "     \n",
    "    \n",
    "        # L2 regularization\n",
    "        if model.l2_reg:\n",
    "            l2=0\n",
    "            for p in model.parameters():\n",
    "                l2 = l2 + (p**2).sum()\n",
    "                loss = loss + lambda_l2 * l2\n",
    "\n",
    "        # Backpropagation on optimizing for loss\n",
    "        # 1. Sets gradients as 0 \n",
    "        # 2. Compute the gradients using back_prop\n",
    "        # 3. update the parameters using the gradients from step 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        _batch_size = len(X)\n",
    "\n",
    "        correct += _correct\n",
    "\n",
    "\n",
    "        size += _batch_size\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "\n",
    "    correct/=size\n",
    "    print(f\"Train : \\nAccuracy: {(100*correct):>0.1f}%\")\n",
    "\n",
    "    f1 = f1_score(y_true.cpu(), y_pred.cpu(), average='macro')\n",
    "    print(f\"F1 score: {(100*f1):>0.1f}%\")\n",
    "\n",
    "def validation(dataloader, model, loss_fn):\n",
    "\n",
    "    # Total size of dataset for reference\n",
    "    size = 0\n",
    "\n",
    "    # Setting the model under evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    _correct = 0\n",
    "    _batch_size = 0\n",
    "    \n",
    "    y_true = torch.tensor(()).to(device)\n",
    "    y_pred = torch.tensor(()).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Gives X , y for each batch\n",
    "        for batch , (X, y) in enumerate(dataloader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            model.to(device)\n",
    "            pred = model.forward(X)\n",
    "            \n",
    "            y_true = torch.cat((y_true, y), 0)\n",
    "            y_pred = torch.cat((y_pred, pred.argmax(1)), 0)\n",
    "\n",
    "            loss_fn(pred, y).item()\n",
    "            _batch_size = len(X)\n",
    "\n",
    "            _correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            correct += _correct\n",
    "\n",
    "            size+=_batch_size\n",
    "\n",
    "\n",
    "    ## Calculating Accuracy based on how many y match with y_pred\n",
    "    correct /= size\n",
    "    \n",
    "    ## calculating f1 score\n",
    "    f1 = f1_score(y_true.cpu(), y_pred.cpu(), average='macro')\n",
    "\n",
    "    print(f\"Validation : \\nAccuracy: {(100*correct):>0.1f}%\")\n",
    "    print(f\"F1 score: {(100*f1):>0.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aeb6d0",
   "metadata": {},
   "source": [
    "### 7.5.2 Experiment with your model (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69876914",
   "metadata": {},
   "source": [
    "Use the MNIST dataset and evaluation code from the previous assignment to run some experiments. Run the following experiments:\n",
    "\n",
    "1. Shallow network (not more than 1 hidden layer)\n",
    "1. Shallow regularized network\n",
    "1. Deep network (at least 3 hidden layers)\n",
    "1. Deep regularized network\n",
    "\n",
    "Report Accuracy and $F_1$ metrics for your experiments and discuss your results. What did you expect to see and what did you end up seeing.\n",
    "\n",
    "**NOTE**: You can choose how you use regularization. Ideally you would experiment with various parameters for this regularization, the 4 listed variants are merely what you must cover as a minimum. Report results for all your experiments concisely in a table.\n",
    "\n",
    "**NOTE 2**: Make sure to report your metrics on the training and evaluation/heldout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11d423b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# DO NOT CHANGE THE CODE IN THIS CELL EXCEPT FOR THE BATCH SIZE IF NECESSARY\n",
    "transform_fn = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.7,), (0.7,)),])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_fn)\n",
    "train_dl = torch.utils.data.DataLoader(mnist_train, batch_size=16, shuffle=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_fn)\n",
    "test_dl = torch.utils.data.DataLoader(mnist_test, batch_size=16, shuffle=False)\n",
    "\n",
    "# Use the above data for your experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9dce2ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run your experiments\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "hidden_dim = 512\n",
    "dropout_p = 0.5\n",
    "inp = 28*28\n",
    "out = 10\n",
    "lambda_l2 = 0.001\n",
    "epochs = 5\n",
    "\n",
    "initial_arr = [True, False]\n",
    "arr = np.random.choice(initial_arr, size=hidden_dim)\n",
    "bool_list = list(map(bool, arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c174025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow Model \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.366033  [    0]\n",
      "loss: 1.187685  [ 6400]\n",
      "loss: 0.541177  [12800]\n",
      "loss: 0.563532  [19200]\n",
      "loss: 0.276086  [25600]\n",
      "loss: 0.175800  [32000]\n",
      "loss: 0.110824  [38400]\n",
      "loss: 0.453283  [44800]\n",
      "loss: 0.363207  [51200]\n",
      "loss: 0.244357  [57600]\n",
      "Train : \n",
      "Accuracy: 83.4%\n",
      "F1 score: 83.2%\n",
      "Validation : \n",
      "Accuracy: 90.5%\n",
      "F1 score: 90.4%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.053905  [    0]\n",
      "loss: 0.236218  [ 6400]\n",
      "loss: 0.102433  [12800]\n",
      "loss: 0.152733  [19200]\n",
      "loss: 0.029574  [25600]\n",
      "loss: 0.080245  [32000]\n",
      "loss: 0.359718  [38400]\n",
      "loss: 0.490714  [44800]\n",
      "loss: 0.184518  [51200]\n",
      "loss: 0.568807  [57600]\n",
      "Train : \n",
      "Accuracy: 91.6%\n",
      "F1 score: 91.5%\n",
      "Validation : \n",
      "Accuracy: 92.9%\n",
      "F1 score: 92.8%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.407561  [    0]\n",
      "loss: 0.127263  [ 6400]\n",
      "loss: 0.614773  [12800]\n",
      "loss: 0.143562  [19200]\n",
      "loss: 0.223976  [25600]\n",
      "loss: 0.109319  [32000]\n",
      "loss: 0.035222  [38400]\n",
      "loss: 0.117323  [44800]\n",
      "loss: 0.176518  [51200]\n",
      "loss: 0.059757  [57600]\n",
      "Train : \n",
      "Accuracy: 93.7%\n",
      "F1 score: 93.6%\n",
      "Validation : \n",
      "Accuracy: 94.9%\n",
      "F1 score: 94.8%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.195550  [    0]\n",
      "loss: 0.039157  [ 6400]\n",
      "loss: 0.062578  [12800]\n",
      "loss: 0.208400  [19200]\n",
      "loss: 0.012832  [25600]\n",
      "loss: 0.149089  [32000]\n",
      "loss: 0.205032  [38400]\n",
      "loss: 0.113553  [44800]\n",
      "loss: 0.211684  [51200]\n",
      "loss: 0.047372  [57600]\n",
      "Train : \n",
      "Accuracy: 95.0%\n",
      "F1 score: 94.9%\n",
      "Validation : \n",
      "Accuracy: 95.4%\n",
      "F1 score: 95.3%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.198965  [    0]\n",
      "loss: 0.064056  [ 6400]\n",
      "loss: 0.116756  [12800]\n",
      "loss: 0.077983  [19200]\n",
      "loss: 0.012963  [25600]\n",
      "loss: 0.012105  [32000]\n",
      "loss: 0.320374  [38400]\n",
      "loss: 0.011886  [44800]\n",
      "loss: 0.022028  [51200]\n",
      "loss: 0.062945  [57600]\n",
      "Train : \n",
      "Accuracy: 95.7%\n",
      "F1 score: 95.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "shallow = Model(inp, hidden_dim, out, 1, bool_list, 0, l2_reg = False).to(device)\n",
    "optimizer = torch.optim.SGD(shallow.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "print(\"Shallow Model \\n\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dl, shallow, loss_function, optimizer, lambda_l2)\n",
    "    validation(test_dl, shallow, loss_function)     \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cd3b67da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow Regularization Model \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.082001  [    0]\n",
      "loss: 0.094270  [ 6400]\n",
      "loss: 0.203187  [12800]\n",
      "loss: 0.217602  [19200]\n",
      "loss: 0.701317  [25600]\n",
      "loss: 0.155076  [32000]\n",
      "loss: 0.039931  [38400]\n",
      "loss: 0.033724  [44800]\n",
      "loss: 0.117633  [51200]\n",
      "loss: 0.013330  [57600]\n",
      "Train : \n",
      "Accuracy: 96.7%\n",
      "F1 score: 96.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.043046  [    0]\n",
      "loss: 0.006882  [ 6400]\n",
      "loss: 0.156729  [12800]\n",
      "loss: 0.052657  [19200]\n",
      "loss: 0.145447  [25600]\n",
      "loss: 0.009354  [32000]\n",
      "loss: 0.474807  [38400]\n",
      "loss: 0.086060  [44800]\n",
      "loss: 0.048467  [51200]\n",
      "loss: 0.464221  [57600]\n",
      "Train : \n",
      "Accuracy: 96.7%\n",
      "F1 score: 96.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.050266  [    0]\n",
      "loss: 0.034112  [ 6400]\n",
      "loss: 0.104641  [12800]\n",
      "loss: 0.024506  [19200]\n",
      "loss: 0.117050  [25600]\n",
      "loss: 0.117034  [32000]\n",
      "loss: 0.157268  [38400]\n",
      "loss: 0.050026  [44800]\n",
      "loss: 0.017901  [51200]\n",
      "loss: 0.095434  [57600]\n",
      "Train : \n",
      "Accuracy: 96.7%\n",
      "F1 score: 96.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.100168  [    0]\n",
      "loss: 0.057893  [ 6400]\n",
      "loss: 0.099071  [12800]\n",
      "loss: 0.039725  [19200]\n",
      "loss: 0.022453  [25600]\n",
      "loss: 0.295633  [32000]\n",
      "loss: 0.063661  [38400]\n",
      "loss: 0.072424  [44800]\n",
      "loss: 0.159967  [51200]\n",
      "loss: 0.017451  [57600]\n",
      "Train : \n",
      "Accuracy: 96.7%\n",
      "F1 score: 96.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.239668  [    0]\n",
      "loss: 0.017080  [ 6400]\n",
      "loss: 0.052335  [12800]\n",
      "loss: 0.026835  [19200]\n",
      "loss: 0.101850  [25600]\n",
      "loss: 0.046270  [32000]\n",
      "loss: 0.114237  [38400]\n",
      "loss: 0.049500  [44800]\n",
      "loss: 0.044793  [51200]\n",
      "loss: 0.196309  [57600]\n",
      "Train : \n",
      "Accuracy: 96.7%\n",
      "F1 score: 96.7%\n",
      "Validation : \n",
      "Accuracy: 96.3%\n",
      "F1 score: 96.2%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "shallow_reg = Model(inp, hidden_dim, out, 1, bool_list, dropout_p, l2_reg = True).to(device)\n",
    "optimizer = torch.optim.SGD(shallow_reg.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Shallow Regularization Model \\n\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dl, shallow, loss_function, optimizer, lambda_l2)\n",
    "    validation(test_dl, shallow, loss_function)     \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ad00b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Model \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.312886  [    0]\n",
      "loss: 2.208130  [ 6400]\n",
      "loss: 1.695760  [12800]\n",
      "loss: 1.027298  [19200]\n",
      "loss: 0.623616  [25600]\n",
      "loss: 0.999520  [32000]\n",
      "loss: 0.413017  [38400]\n",
      "loss: 0.548535  [44800]\n",
      "loss: 0.384042  [51200]\n",
      "loss: 0.367201  [57600]\n",
      "Train : \n",
      "Accuracy: 70.1%\n",
      "F1 score: 70.4%\n",
      "Validation : \n",
      "Accuracy: 89.0%\n",
      "F1 score: 88.9%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.333760  [    0]\n",
      "loss: 0.401197  [ 6400]\n",
      "loss: 0.429575  [12800]\n",
      "loss: 0.076977  [19200]\n",
      "loss: 0.352355  [25600]\n",
      "loss: 0.257197  [32000]\n",
      "loss: 0.177038  [38400]\n",
      "loss: 0.389601  [44800]\n",
      "loss: 0.151102  [51200]\n",
      "loss: 0.236277  [57600]\n",
      "Train : \n",
      "Accuracy: 90.7%\n",
      "F1 score: 90.6%\n",
      "Validation : \n",
      "Accuracy: 92.0%\n",
      "F1 score: 91.9%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.430705  [    0]\n",
      "loss: 0.115080  [ 6400]\n",
      "loss: 0.092994  [12800]\n",
      "loss: 0.085429  [19200]\n",
      "loss: 0.048860  [25600]\n",
      "loss: 0.024009  [32000]\n",
      "loss: 0.096219  [38400]\n",
      "loss: 0.159472  [44800]\n",
      "loss: 0.443125  [51200]\n",
      "loss: 0.262067  [57600]\n",
      "Train : \n",
      "Accuracy: 93.7%\n",
      "F1 score: 93.6%\n",
      "Validation : \n",
      "Accuracy: 94.8%\n",
      "F1 score: 94.7%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.151047  [    0]\n",
      "loss: 0.117707  [ 6400]\n",
      "loss: 0.016947  [12800]\n",
      "loss: 0.692262  [19200]\n",
      "loss: 0.056085  [25600]\n",
      "loss: 0.115220  [32000]\n",
      "loss: 0.230221  [38400]\n",
      "loss: 0.045741  [44800]\n",
      "loss: 0.072103  [51200]\n",
      "loss: 0.008987  [57600]\n",
      "Train : \n",
      "Accuracy: 95.1%\n",
      "F1 score: 95.1%\n",
      "Validation : \n",
      "Accuracy: 95.0%\n",
      "F1 score: 94.9%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.113016  [    0]\n",
      "loss: 0.074354  [ 6400]\n",
      "loss: 0.180087  [12800]\n",
      "loss: 0.023891  [19200]\n",
      "loss: 0.039050  [25600]\n",
      "loss: 0.025834  [32000]\n",
      "loss: 0.010161  [38400]\n",
      "loss: 0.306615  [44800]\n",
      "loss: 0.303669  [51200]\n",
      "loss: 0.023743  [57600]\n",
      "Train : \n",
      "Accuracy: 95.9%\n",
      "F1 score: 95.9%\n",
      "Validation : \n",
      "Accuracy: 96.2%\n",
      "F1 score: 96.1%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "deep = Model(inp, hidden_dim, out, 3, bool_list, 0, l2_reg = False).to(device)\n",
    "optimizer = torch.optim.SGD(deep.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Deep Model \\n\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dl, deep, loss_function, optimizer, lambda_l2)\n",
    "    validation(test_dl, deep, loss_function)     \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7bedf4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Regularization Model \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 7.100754  [    0]\n",
      "loss: 6.524984  [ 6400]\n",
      "loss: 6.031408  [12800]\n",
      "loss: 4.896721  [19200]\n",
      "loss: 4.210218  [25600]\n",
      "loss: 3.694695  [32000]\n",
      "loss: 3.606270  [38400]\n",
      "loss: 2.964967  [44800]\n",
      "loss: 2.655102  [51200]\n",
      "loss: 2.294011  [57600]\n",
      "Train : \n",
      "Accuracy: 53.2%\n",
      "F1 score: 52.7%\n",
      "Validation : \n",
      "Accuracy: 84.5%\n",
      "F1 score: 84.2%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.456746  [    0]\n",
      "loss: 2.366848  [ 6400]\n",
      "loss: 2.029624  [12800]\n",
      "loss: 1.724416  [19200]\n",
      "loss: 1.575170  [25600]\n",
      "loss: 1.855827  [32000]\n",
      "loss: 1.664716  [38400]\n",
      "loss: 1.446649  [44800]\n",
      "loss: 1.354885  [51200]\n",
      "loss: 1.444786  [57600]\n",
      "Train : \n",
      "Accuracy: 84.8%\n",
      "F1 score: 84.5%\n",
      "Validation : \n",
      "Accuracy: 87.4%\n",
      "F1 score: 87.2%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.179937  [    0]\n",
      "loss: 1.120596  [ 6400]\n",
      "loss: 1.423028  [12800]\n",
      "loss: 1.171690  [19200]\n",
      "loss: 0.939265  [25600]\n",
      "loss: 0.892074  [32000]\n",
      "loss: 0.926099  [38400]\n",
      "loss: 0.963740  [44800]\n",
      "loss: 0.770604  [51200]\n",
      "loss: 0.890660  [57600]\n",
      "Train : \n",
      "Accuracy: 88.6%\n",
      "F1 score: 88.5%\n",
      "Validation : \n",
      "Accuracy: 91.3%\n",
      "F1 score: 91.2%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.842784  [    0]\n",
      "loss: 0.677503  [ 6400]\n",
      "loss: 1.030301  [12800]\n",
      "loss: 0.863781  [19200]\n",
      "loss: 1.402541  [25600]\n",
      "loss: 0.730378  [32000]\n",
      "loss: 0.662980  [38400]\n",
      "loss: 0.572384  [44800]\n",
      "loss: 0.715640  [51200]\n",
      "loss: 0.818981  [57600]\n",
      "Train : \n",
      "Accuracy: 90.3%\n",
      "F1 score: 90.2%\n",
      "Validation : \n",
      "Accuracy: 93.0%\n",
      "F1 score: 92.9%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.628066  [    0]\n",
      "loss: 0.768000  [ 6400]\n",
      "loss: 1.077873  [12800]\n",
      "loss: 0.555147  [19200]\n",
      "loss: 0.784580  [25600]\n",
      "loss: 0.825213  [32000]\n",
      "loss: 0.605156  [38400]\n",
      "loss: 0.504273  [44800]\n",
      "loss: 0.652206  [51200]\n",
      "loss: 0.565660  [57600]\n",
      "Train : \n",
      "Accuracy: 91.0%\n",
      "F1 score: 90.9%\n",
      "Validation : \n",
      "Accuracy: 92.3%\n",
      "F1 score: 92.2%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "deep_reg = Model(inp, hidden_dim, out, 3, bool_list, dropout_p, l2_reg = True).to(device)\n",
    "optimizer = torch.optim.SGD(deep_reg.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Deep Regularization Model \\n\")\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dl, deep_reg, loss_function, optimizer, lambda_l2)\n",
    "    validation(test_dl, deep_reg, loss_function)     \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3cfbd9",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Model</th>\n",
    "    <th>Train Accuracy</th>\n",
    "    <th>Train F1 Score</th>  \n",
    "    <th>Validation Accuracy</th>\n",
    "    <th>Validation F1 Score</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Shallow network (not more than 1 hidden layer)</td>\n",
    "    <td>95.7%</td>\n",
    "    <td>95.7%</td>\n",
    "    <td>96.3%</td>\n",
    "    <td>96.2%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Shallow regularized network</td>\n",
    "    <td>96.7%</td>\n",
    "    <td>96.7%</td>\n",
    "    <td>96.3%</td>\n",
    "    <td>96.2%</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Deep network (at least 3 hidden layers)</td>\n",
    "    <td>95.9%</td>\n",
    "    <td>95.9%</td>\n",
    "    <td>96.2%</td>\n",
    "    <td>96.1%</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Deep regularized network</td>\n",
    "    <td>91.0%</td>\n",
    "    <td>90.9%</td>\n",
    "    <td>92.3%</td>\n",
    "    <td>92.2%</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Shallow network and shallow regularized network has almost same validation accuracy. We thought that after applying regularization to the model, it will improve generalisation and hence improve validation accuracy but it remains same.<br> \n",
    "Deep network and shallow network has almost same validation accuracy and there is no much difference observed after adding extra layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b43f4",
   "metadata": {},
   "source": [
    "### 7.5.3 Get the best model! (1 + 1 point (bonus))\n",
    "\n",
    "* Present your model during a tutorial session. Justify your decisions when designing your model/solution.\n",
    "* If you achieve one of the top N results, you get yet another extra point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following is the best model(Shallow Regularized Network) from our experiments with validation accuracy of 96.3%\n",
    "\n",
    "hidden_dim = 512\n",
    "dropout_p = 0.5\n",
    "inp = 28*28\n",
    "out = 10\n",
    "shallow_reg = Model(inp, hidden_dim, out, 1, bool_list, dropout_p, l2_reg = True).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
