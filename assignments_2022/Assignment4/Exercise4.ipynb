{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.a Building your own feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added a `ReLU` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$ \\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = ReLU(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c) $ \n",
    "\n",
    "$ \\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{$ softmax $}( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate the ReLU, Softmax or the forward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function as defined in the lecture\n",
    "    Input: an array of numbers\n",
    "    Output: ReLU(x)\n",
    "    \"\"\"\n",
    "    for i in range(len(x)):\n",
    "        x[i] = max(0, x[i])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implement the `softmax` function as defined in the lecture\n",
    "    \"\"\"\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        # Initialize each layer as a random matrix of the\n",
    "        # appropriate dimensions\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        setattr(self, f\"layer_input\", np.random.rand(hidden_dim, input_dim))\n",
    "        setattr(self, f\"bias_input\", np.random.rand(hidden_dim))\n",
    "        \n",
    "        for i in range(hidden_size):\n",
    "            setattr(self, f\"layer_{i}\", np.random.rand(hidden_dim, hidden_dim))\n",
    "            setattr(self, f\"bias_{i}\", np.random.rand(hidden_dim))\n",
    "        \n",
    "        setattr(self, \"layer_output\", np.random.rand(output_dim, hidden_dim))\n",
    "        setattr(self, f\"bias_output\", np.random.rand(output_dim))\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \n",
    "        Note: Remember to apply the ReLU and add the bias for each layer\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass of the network,\n",
    "        # i.e. calculate `y` from an input `x`\n",
    "        # Remember that each layer's output is calculated by\n",
    "        # f^(i) = ReLU(W_i^T * f^(i-1) + b_i)\n",
    "        res = x\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        \n",
    "        res = relu(np.dot(getattr(self, \"layer_input\"), res) + getattr(self, \"bias_input\"))\n",
    "        \n",
    "        for i in range(self.hidden_size):\n",
    "            res = relu(np.dot(getattr(self, f\"layer_{i}\"), res) + getattr(self, f\"bias_{i}\"))\n",
    "        \n",
    "        res = softmax(np.dot(getattr(self, \"layer_output\"), res) + getattr(self, \"bias_output\"))\n",
    "        \n",
    "        ## SOLUTION ##\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87832686, 0.12167314])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# A configuration that reflects the example from the lecture\n",
    "# i.e. our input is of size 2, our intermediate layers are also of size 2,\n",
    "# and we will only have 1 hidden layer.\n",
    "network = FFNetwork(2, 2, 2, 1)\n",
    "network.forward([1.,0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Do not expect a correct output at this stage, you are simply building the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our setup also allows us to create larger networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15216092, 0.84783908])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "network = FFNetwork(2, 3, 2, 4)\n",
    "network.forward([1.,0.]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sanity checks:\n",
    "\n",
    "1. You should be seeing the number of units you specified as output units in your output.\n",
    "1. The numbers in your outputs should be in the range $[0,1]$\n",
    "1. The numbers should add up to $1$\n",
    "1. Varying the structure of the network should not break its functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.b Implementing a feed-forward network using `torch`\n",
    "\n",
    "### 4.4.b.1 Creating the network (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will be using the `nn` module of `torch`, which contains modules representing types of layers. In your case, the specific relevant module would be that of a *fully connected linear layer*.\n",
    "\n",
    "We will also be using the `nn.functional` module to take advantage of the built in functions for ReLU and Softmax. In this exercise, you are allowed to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A `torch` version of the network implemented for 4.3.b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        super(TorchFFNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        setattr(self, f\"layer_input\", nn.Linear(input_dim, hidden_dim, bias=True))\n",
    "        \n",
    "        for i in range(hidden_size):\n",
    "            setattr(self, f\"layer_{i}\", nn.Linear(hidden_dim, hidden_dim, bias=True))\n",
    "        \n",
    "        setattr(self, \"layer_output\", nn.Linear(hidden_dim, output_dim, bias=True))\n",
    "\n",
    "        ## SOLUTION ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## SOLUTION ##\n",
    "        res = x\n",
    "\n",
    "        res = getattr(self, \"layer_input\")(res)\n",
    "        res = F.relu(res)\n",
    "\n",
    "        for i in range(self.hidden_size):\n",
    "            res = getattr(self, f\"layer_{i}\")(res)\n",
    "            res = F.relu(res)\n",
    "\n",
    "        res = getattr(self, \"layer_output\")(res)\n",
    "        res = F.softmax(res)\n",
    "        \n",
    "        return res\n",
    "        ## SOLUTION ##\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation, once more, needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_network = TorchFFNetwork(2, 2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6288, 0.3712])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carbon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch_network(torch.tensor([1.,0.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `forward` method is automatically called when you call your network object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.b.2 Training your network (1 point)\n",
    "\n",
    "Even though we have not covered how training actually works, we will proceed with the training of the a neural network as a blackbox procedure and we will later on learn the internals of the training process (and even implement them ourselves!).\n",
    "\n",
    "For now, train a neural network (the one you created above) to learn the XOR operation. You are to create a neural network with the appropriate number of input variables, an intermediate hidden layer with 2 units and an output layer with 2 units.\n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimization-loop). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- You are to train the network until the network learns the operation. Remember to set your random seeds so the results are reproducible.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD. It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training X, where each instance includes an x1 and an x2, (where the operation is defined as x1 XOR x2)\n",
    "training_x = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "# We have only covered softmax in the lecture, so we format the output as follows:\n",
    "training_y = [[1,0], [0,1], [0,1], [1,0]]\n",
    "\n",
    "# The Y is formatted such that the its first element corresponds to the probability of the XOR resulting in a 0\n",
    "# and the second element to the probability of the XOR resulting in a 1\n",
    "\n",
    "################################################################\n",
    "# TODO: Adapt the training set so it can be used with `pytorch`\n",
    "################################################################\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "\n",
    "training_x = torch.tensor(training_x, dtype=torch.float, requires_grad=True)\n",
    "training_y = torch.tensor(training_y, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "dataset = TensorDataset(training_x, training_y)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model from the previous class and pick a learning rate\n",
    "torch.manual_seed(42)\n",
    "model = TorchFFNetwork(2, 2, 2, 2)\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, model, loss_fn, optimizer):\n",
    "    # TODO: Implement\n",
    "    for batch, (xs, ys) in enumerate(data):\n",
    "        pred = model.forward(xs)\n",
    "        loss = loss_fn(pred, ys)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(xs)\n",
    "        print(f\"loss: {loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.706652\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.706633\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.706613\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.706594\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.706575\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.706556\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.706536\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.706517\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.706498\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.706478\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carbon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()}\n",
    "], lr=learning_rate)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(data_loader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
