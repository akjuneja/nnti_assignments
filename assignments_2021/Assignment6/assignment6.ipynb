{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.2 (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will continue the concept of toy library that you started in the previous assignment. As the main topic of this week's lecture was backpropagation, you are asked to perform backpropagation on a neural network model that you will build in this exercise.  \n",
    "  \n",
    "In this toy library, we are not implementing the functionalities of autograd or any other automatic differentiation. Still it will be extremely helpful for you to know the basics about how the PyTorch autograd functionality works (e.g. for checking your implementation of gradient calculations). A good starting point would be [PyTorch autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).   \n",
    "\n",
    "All classes that you implemented in the previous week must now have a `grad` function which would compute and return gradients. The `grad` function in the classes of the following *loss* funtions (`MSELoss` and `CrossEntropyLoss`) must compute gradients of the loss w.r.t. its input. The `grad` function for *activation functions* must take the incoming gradient (possibly from the previous layer or the loss function) and compute gradients of the loss w.r.t. its input. The `grad` function for *layers* (in this exercise we have only `Linear` layer) must take the incoming gradient and compute gradients of the loss w.r.t. both its input and weights (you can ignore computing the gradients w.r.t. biases).\n",
    "\n",
    "For each gradient calculation, we are providing some low-dimensional data. After you finish the implementation of each `grad` function, simply run the corresponding cell (**do not** change the contents of these cells). To check for the correctness of the implementation, we ask you to *call* the corresponding function from PyTorch on the same input data, compute gradients, and compare them with the gradients from your implementation. If you have a correct solution, then they must be the same (or maybe with some very small <$10^{-3}$ differences).\n",
    "\n",
    "We are providing the correct solution for the previous assignment so that you can still work on this one, even if you had some mistakes in your solution. Simply replace the files or work from a different directory.\n",
    "\n",
    "Please remember that everything is processed in minibatches and gradients must be calculated accordingly. The input for each of the model components has dimensions of `N*D` where `N` is the number of datapoints in minibatch and `D` is the number of features. Of course, all the gradient computations must, ideally, be implemented in vectorized form (without using any loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "torch.manual_seed(23)\n",
    "random.seed(23)\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2.1 Implement gradient calculation for MSE loss (0.5 points)\n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "[0 1 2]\n",
      "true values\n",
      "[1 3 3]\n",
      "MSE loss\n",
      "2.0\n",
      "MSE gradient\n",
      "[-0.66666667 -1.33333333 -0.66666667]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([0, 1, 2])\n",
    "y_true = np.array([1, 3, 3])\n",
    "loss = MSELoss()\n",
    "\n",
    "print('predictions')\n",
    "print(y_pred)\n",
    "print('true values')\n",
    "print(y_true)\n",
    "\n",
    "print('MSE loss')\n",
    "print(loss(y_pred, y_true))\n",
    "\n",
    "print('MSE gradient')\n",
    "print(loss.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "tensor([0., 1., 2.], requires_grad=True)\n",
      "true values\n",
      "tensor([1., 3., 3.])\n",
      "MSE loss\n",
      "2.0\n",
      "MSE gradient\n",
      "[-0.6666667 -1.3333334 -0.6666667]\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.tensor([0, 1, 2], requires_grad=True, dtype=torch.float32)\n",
    "y_true = torch.tensor([1, 3, 3], dtype=torch.float32)\n",
    "\n",
    "print('predictions')\n",
    "print(y_pred)\n",
    "print('true values')\n",
    "print(y_true)\n",
    "\n",
    "print('MSE loss')\n",
    "# TODO - Implement: your code goes here\n",
    "loss = nn.MSELoss()\n",
    "output_loss = loss(y_true, y_pred)\n",
    "print(output_loss.item())\n",
    "\n",
    "print('MSE gradient')\n",
    "# TODO - Implement: your code goes here\n",
    "output_loss.backward()\n",
    "print(y_pred.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2.2 Implement gradient calculation for Cross Entropy Loss (1.5 points)\n",
    "First prove that the partial derivative of the loss w.r.t. one of the input variables is <br> $\\frac{\\delta L}{\\delta o_i} = p_i - y_i$  \n",
    "where <br> $o_i$ - one of the input variables,   \n",
    "$p_i$ - probability for that input variable calculated using softmax,   \n",
    "$y_i$ - label for that input variable ($y_i \\in \\{0, 1\\}$).  \n",
    "For simplicity of the proof, you can prove it for just one datapoint, but in the code, you should properly extrapolate it for computing the gradients for the whole minibatch (`N` datapoints).  \n",
    "\n",
    "Write the solution in the markdown cell below.\n",
    "  \n",
    "Please remember that a typical Cross Entropy Loss implementation, including ours, implicitly applies Softmax before calculating the CE loss.\n",
    "  \n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "[[0.4  0.35 0.71 0.3 ]\n",
      " [0.01 0.01 0.01 0.65]]\n",
      "targets\n",
      "[[0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "cross entropy loss\n",
      "1.0391157169091105\n",
      "gradient of the cross entropy loss\n",
      "[[ 0.11849768  0.11271848 -0.33843729  0.10722113]\n",
      " [ 0.10211415  0.10211415  0.10211415 -0.30634246]]\n"
     ]
    }
   ],
   "source": [
    "ce_loss = CrossEntropy(average=True)\n",
    "predictions = np.array([[0.4,0.35,0.71,0.30],\n",
    "                        [0.01,0.01,0.01,0.65]])\n",
    "targets = np.array([[0,0,1,0],\n",
    "                  [0,0,0,1]])\n",
    "\n",
    "print('predictions')\n",
    "print(predictions)\n",
    "print('targets')\n",
    "print(targets)\n",
    "\n",
    "print('cross entropy loss')\n",
    "print(ce_loss(predictions, targets))\n",
    "\n",
    "print('gradient of the cross entropy loss')\n",
    "print(ce_loss.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "tensor([[0.4000, 0.3500, 0.7100, 0.3000],\n",
      "        [0.0100, 0.0100, 0.0100, 0.6500]], requires_grad=True)\n",
      "targets\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "cross entropy loss\n",
      "1.0391156673431396\n",
      "gradient of the cross entropy loss\n",
      "[[ 0.11849768  0.11271848 -0.3384373   0.10722113]\n",
      " [ 0.10211416  0.10211416  0.10211416 -0.30634245]]\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.tensor([[0.4,0.35,0.71,0.30],\n",
    "                            [0.01,0.01,0.01,0.65]], dtype=torch.float32, requires_grad=True)\n",
    "targets = torch.tensor([[0,0,1,0],\n",
    "                        [0,0,0,1]], dtype=torch.float32)\n",
    "t = torch.argmax(targets, dim=1)\n",
    "print('predictions')\n",
    "print(predictions)\n",
    "print('targets')\n",
    "print(targets)\n",
    "\n",
    "print('cross entropy loss')\n",
    "# TODO - Implement: your code goes here\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output_loss = loss(predictions, t)\n",
    "print(output_loss.item())\n",
    "\n",
    "print('gradient of the cross entropy loss')\n",
    "# TODO - Implement: your code goes here\n",
    "output_loss.backward()\n",
    "print(predictions.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2.3 Implement gradient calculation for linear layer (1.5 points)\n",
    "\n",
    "First prove that $\\frac{\\delta L}{\\delta X} = \\frac{\\delta L}{\\delta Y} W^T$ and $\\frac{\\delta L}{\\delta W} = X^T \\frac{\\delta L}{\\delta Y}$  \n",
    "where $Y = XW$ <br> (X - input data matrix of dimension `N * in_features` and W is a weight matrix of dimension `in_features * out_features`),  \n",
    "$\\frac{\\delta L}{\\delta Y}$ is the incoming gradient of dimension `N * out_features` (e.g. from the loss function that is applied on the outputs of the linear layer).  \n",
    "\n",
    "Write the solution in the markdown cell below.\n",
    "\n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data\n",
      "[[ 0.66698806  0.02581308 -0.77761941  0.94863382  0.70167179]\n",
      " [-1.05108156 -0.36754812 -1.13745969 -1.32214752  1.77225828]\n",
      " [-0.34745899  0.67014016  0.32227152  0.06034293 -1.04345   ]\n",
      " [-1.00994188  0.44173637  1.12887685 -1.83806777 -0.93876863]]\n",
      "[[-0.01009203  0.05226856]\n",
      " [ 0.0269081   0.04060593]\n",
      " [ 0.01205532 -0.04762548]\n",
      " [-0.00681334  0.06336241]\n",
      " [ 0.00868168 -0.06116274]]\n",
      "output of the linear layer\n",
      "[[ 0.0549832   0.11302227]\n",
      " [ 0.08216551 -0.18497617]\n",
      " [ 0.08671981  0.08423143]\n",
      " [ 0.1108269  -0.12477559]]\n",
      "(4, 2)\n",
      "(2, 5)\n",
      "gradient w.r.t weights\n",
      "[[-1.74149438 -1.74149438]\n",
      " [ 0.7701415   0.7701415 ]\n",
      " [-0.46393073 -0.46393073]\n",
      " [-2.15123853 -2.15123853]\n",
      " [ 0.49171144  0.49171144]]\n",
      "gradient w.r.t. inputs\n",
      "[[ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]]\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 4\n",
    "in_features = 5\n",
    "out_features = 2\n",
    "minibatch = np.random.randn(minibatch_size, in_features)\n",
    "print('input data')\n",
    "print(minibatch)\n",
    "\n",
    "layer = Linear(in_features, out_features)\n",
    "print(layer.weights)\n",
    "print('output of the linear layer')\n",
    "print(layer(minibatch))\n",
    "\n",
    "in_gradient = np.ones((minibatch_size, out_features,))\n",
    "gradient_weights, gradient_input = layer.grad(in_gradient)\n",
    "print('gradient w.r.t weights')\n",
    "print(gradient_weights)\n",
    "print('gradient w.r.t. inputs')\n",
    "print(gradient_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, to get the same output, the *weights* and *biases* of the `Linear` layer instantiated above and the `Linear` layer from PyTorch must be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data\n",
      "tensor([[ 0.6670,  0.0258, -0.7776,  0.9486,  0.7017],\n",
      "        [-1.0511, -0.3675, -1.1375, -1.3221,  1.7723],\n",
      "        [-0.3475,  0.6701,  0.3223,  0.0603, -1.0434],\n",
      "        [-1.0099,  0.4417,  1.1289, -1.8381, -0.9388]], requires_grad=True)\n",
      "output of the linear layer\n",
      "tensor([[ 0.0550,  0.1130],\n",
      "        [ 0.0822, -0.1850],\n",
      "        [ 0.0867,  0.0842],\n",
      "        [ 0.1108, -0.1248]], grad_fn=<SqueezeBackward1>)\n",
      "gradient w.r.t weights\n",
      "[[-1.7414944  -1.7414944 ]\n",
      " [ 0.7701415   0.7701415 ]\n",
      " [-0.46393073 -0.46393073]\n",
      " [-2.1512384  -2.1512384 ]\n",
      " [ 0.4917115   0.4917115 ]]\n",
      "gradient w.r.t. inputs\n",
      "[[ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]\n",
      " [ 0.04217654  0.06751403 -0.03557016  0.05654907 -0.05248106]]\n"
     ]
    }
   ],
   "source": [
    "print('input data')\n",
    "minibatch = torch.tensor(minibatch, dtype=torch.float32, requires_grad=True)\n",
    "print(minibatch)\n",
    "\n",
    "print('output of the linear layer')\n",
    "# TODO - Implement: your code goes here\n",
    "nn_linear = nn.Linear(in_features, out_features)\n",
    "nn_linear.weight.data = torch.tensor(layer.weights.transpose(), dtype=torch.float32)\n",
    "nn_linear.bias.data = torch.tensor(layer.bias, dtype=torch.float32)\n",
    "out = nn_linear(minibatch.unsqueeze(0))\n",
    "out = out.squeeze(0)\n",
    "print(out)\n",
    "print('gradient w.r.t weights')\n",
    "# TODO - Implement: your code goes here\n",
    "in_grad = torch.tensor(in_gradient)\n",
    "out.backward(gradient=in_grad)\n",
    "print(nn_linear.weight.grad.T.numpy())\n",
    "\n",
    "print('gradient w.r.t. inputs')\n",
    "# TODO - Implement: your code goes here\n",
    "print(minibatch.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2.4 Implement gradient calculation for activation functions (1 point)\n",
    "Check the correctness of the gradients by calculating them on the same data using PyTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52497919 0.42555748 0.62245933 0.7109495  0.5        0.26894142]\n",
      " [0.549834   0.40131234 0.75026011 0.59868766 0.57444252 0.5       ]]\n",
      "[[0.24937604 0.24445831 0.23500371 0.20550031 0.25       0.19661193]\n",
      " [0.24751657 0.24026075 0.18736988 0.24026075 0.24445831 0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.1, -0.3, 0.5, 0.9, 0, -1.0],\n",
    "              [0.2, -0.4, 1.1, 0.4, 0.3, 0]])\n",
    "sigmoid = Sigmoid()\n",
    "print(sigmoid(x))\n",
    "\n",
    "in_gradient = np.ones((2, 6,))\n",
    "print(sigmoid.grad(in_gradient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52497919 0.42555748 0.62245933 0.7109495  0.5        0.26894142]\n",
      " [0.549834   0.40131234 0.75026011 0.59868766 0.57444252 0.5       ]]\n",
      "[[0.24937604 0.24445831 0.23500371 0.20550031 0.25       0.19661193]\n",
      " [0.24751657 0.24026075 0.18736988 0.24026075 0.24445831 0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "# TODO - Implement: your code goes here\n",
    "sigmoid_1 = nn.Sigmoid()\n",
    "output = sigmoid_1(x_torch)\n",
    "print(output.detach().numpy())\n",
    "torch_in = torch.tensor(in_gradient)\n",
    "output.backward(gradient=torch_in)\n",
    "print(x_torch.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09966799 -0.29131261  0.46211716  0.71629787  0.         -0.76159416]\n",
      " [ 0.19737532 -0.37994896  0.80049902  0.37994896  0.29131261  0.        ]]\n",
      "[[0.99006629 0.91513696 0.78644773 0.48691736 1.         0.41997434]\n",
      " [0.96104298 0.85563879 0.35920132 0.85563879 0.91513696 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tanh = Tanh()\n",
    "print(tanh(x))\n",
    "print(tanh.grad(in_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09966799 -0.29131261  0.46211716  0.71629787  0.         -0.76159416]\n",
      " [ 0.19737532 -0.37994896  0.80049902  0.37994896  0.29131261  0.        ]]\n",
      "[[0.99006629 0.91513696 0.78644773 0.48691736 1.         0.41997434]\n",
      " [0.96104298 0.85563879 0.35920132 0.85563879 0.91513696 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "# TODO - Implement: your code goes here\n",
    "tanh_1 = nn.Tanh()\n",
    "output = tanh_1(x_torch)\n",
    "print(output.detach().numpy())\n",
    "torch_in = torch.tensor(in_gradient)\n",
    "output.backward(gradient=torch_in)\n",
    "print(x_torch.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1 -0.   0.5  0.9  0.  -0. ]\n",
      " [ 0.2 -0.   1.1  0.4  0.3  0. ]]\n",
      "[[1. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "print(relu(x))\n",
    "print(relu.grad(in_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.  0.5 0.9 0.  0. ]\n",
      " [0.2 0.  1.1 0.4 0.3 0. ]]\n",
      "[[1. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "# TODO - Implement: your code goes here\n",
    "relu_1 = nn.ReLU()\n",
    "output = relu_1(x_torch)\n",
    "print(output.detach().numpy())\n",
    "torch_in = torch.tensor(in_gradient)\n",
    "output.backward(gradient=torch_in)\n",
    "print(x_torch.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1   -0.003  0.5    0.9    0.    -0.01 ]\n",
      " [ 0.2   -0.004  1.1    0.4    0.3    0.   ]]\n",
      "[[1.   0.01 1.   1.   0.01 0.01]\n",
      " [1.   0.01 1.   1.   1.   0.01]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.1, -0.3, 0.5, 0.9, 0, -1.0],\n",
    "              [0.2, -0.4, 1.1, 0.4, 0.3, 0]])\n",
    "leaky_relu = LeakyReLU()\n",
    "print(leaky_relu(x))\n",
    "print(leaky_relu.grad(in_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1   -0.003  0.5    0.9    0.    -0.01 ]\n",
      " [ 0.2   -0.004  1.1    0.4    0.3    0.   ]]\n",
      "[[1.   0.01 1.   1.   0.01 0.01]\n",
      " [1.   0.01 1.   1.   1.   0.01]]\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "# TODO - Implement: your code goes here\n",
    "x = np.array([[0.1, -0.3, 0.5, 0.9, 0, -1.0],\n",
    "              [0.2, -0.4, 1.1, 0.4, 0.3, 0]])\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "in_gradient = np.ones((2, 6,))\n",
    "lrelu_1 = nn.LeakyReLU(0.01)\n",
    "output = lrelu_1(x_torch)\n",
    "print(output.detach().numpy())\n",
    "torch_in = torch.tensor(in_gradient)\n",
    "output.backward(gradient=torch_in)\n",
    "print(x_torch.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2.5 Implement a model class (1.5 points)\n",
    "Implement a model class which stores a list of components of the model (in this exercise those are only the *layers* and *activation functions*). \n",
    "It must perform the forward pass and also be able to calculate and store the gradients for all the layers, and perform a parameter update step (here we deviate from PyTorch since we don't use *autograd*).  \n",
    "For simplicity, you don't have to compare the value of each parameter of the model with PyTorch implementation, but just check the value of the resultant loss (before and after the parameter update step). We provide all the code, including the code for PyTorch below. You don't have to change the cells below, but just check whether your implementation of the model achieves the same decrease in loss as the equivalent implementation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9648272572841394\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "np.random.seed(123)\n",
    "\n",
    "layer1 = Linear(1000, 100)\n",
    "activation1 = ReLU()\n",
    "layer2 = Linear(100, 10)\n",
    "activation2 = ReLU()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.random.randn(2, 1000)\n",
    "y_true = np.zeros((2, 10,))\n",
    "y_true[0, 4] = 1\n",
    "y_true[1, 1] = 1\n",
    "m = Model([layer1, activation1, layer2, activation2])\n",
    "out = m.forward(x)\n",
    "print(loss(out, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9648274183273315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/xkjzyd4d26l9g8h9y20nxxsc0000gn/T/ipykernel_81111/2412122019.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true_torch = torch.tensor(t, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1000, 100, bias=True)\n",
    "        self.layer2 = nn.Linear(100, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return x\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = Net()\n",
    "y_true_torch = torch.tensor(y_true)\n",
    "t = torch.argmax(y_true_torch, dim=1)\n",
    "with torch.no_grad():\n",
    "    net.layer1.weight.copy_(torch.tensor(layer1.weights).t())\n",
    "    net.layer1.bias.copy_(torch.tensor(layer1.bias[0,:]))\n",
    "    net.layer2.weight.copy_(torch.tensor(layer2.weights).t())\n",
    "    net.layer2.bias.copy_(torch.tensor(layer2.bias[0,:]))\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "out = net(x_torch)\n",
    "y_true_torch = torch.tensor(t, dtype=torch.float32)\n",
    "loss_torch = criterion(out, t)\n",
    "print(loss_torch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04425597  0.04100563  0.04100563  0.04100563 -0.41340392  0.04100563\n",
      "   0.06862488  0.04100563  0.04334649  0.05214843]\n",
      " [ 0.05039602 -0.44326978  0.03906294  0.0682299   0.0592623   0.03906294\n",
      "   0.03906294  0.03906294  0.03906294  0.07006688]]\n",
      "(2, 10)\n",
      "(10, 100)\n",
      "(2, 100)\n",
      "(100, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,10) (2,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hm/xkjzyd4d26l9g8h9y20nxxsc0000gn/T/ipykernel_81111/2015341531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_loss_ours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI_Saarland/Neural_Network 21:22/assignment6/Akhil Juneja_7015523_ Faria Alam_701118_Ex6/model/model.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, grads, lr)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mperforms\u001b[0m \u001b[0mone\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mall\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         '''\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,10) (2,100) "
     ]
    }
   ],
   "source": [
    "print(loss.grad())\n",
    "grads = m.backward(loss.grad())\n",
    "m.update_parameters(grads, 0.001)\n",
    "out = m.forward(x)\n",
    "model_loss_ours = loss(out, y_true)\n",
    "print(model_loss_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8631404638290405\n"
     ]
    }
   ],
   "source": [
    "loss_torch.backward()\n",
    "optimizer.step()\n",
    "\n",
    "out = net(x_torch)\n",
    "model_loss_pt = criterion(out, t).item()\n",
    "print(model_loss_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_loss_ours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hm/xkjzyd4d26l9g8h9y20nxxsc0000gn/T/ipykernel_81111/2683544429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sanity check within some acceptable tolerance level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loss_ours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loss_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_loss_ours' is not defined"
     ]
    }
   ],
   "source": [
    "# sanity check within some acceptable tolerance level\n",
    "\n",
    "np.allclose(model_loss_ours, model_loss_pt, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
