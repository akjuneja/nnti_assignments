{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-malta",
   "metadata": {},
   "source": [
    "# Assignment 10\n",
    "## Exercise 10.2 (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-ethnic",
   "metadata": {},
   "source": [
    "**Note**: This exercise is mostly devoted to the Transformer model which will be described during lecture on 25th of January."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-smoke",
   "metadata": {},
   "source": [
    "In this exercise, you will be implementing `Multi-Head Attention` to solve a toy exercise in sequence modeling. The concept of `Multi-Head Attention` is from the famous paper called [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762), which introduced the Transformer model. Please read the paper carefully and answer the questions below. Understanding the concepts described in this paper will help understanding many modern models in the Neural Networks field and it's also necessary if you choose to work on the NLP project later. \n",
    "\n",
    "If you have troubles understanding the paper you can read the [illustrated transformer blog](https://jalammar.github.io/illustrated-transformer/) first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-athens",
   "metadata": {},
   "source": [
    "i) The biggest benefit of using Transformers instead of RNN and convolution-based models is the possibility to parallelize computations during training. Why parallelization is not possible with RNN and Convolution-based models for sequence processing, but possible with Transformers? *Note*: parallelization can be applied only to the Encoder part of the Trasnformer. (0.5 points)  \n",
    "\n",
    "ii) In explaining the concept of `self-attention` the paper mentions 3 matrices `Q`, `K` and `V` which serve as an input to self-attention mechanism sublayer. Explain how these matrices are computed in the encoder and in the decoder. What role each of these matrices play? (1 point)  \n",
    "\n",
    "iii) How is Multi-Head Attention better than Single-Head Attention? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-fetish",
   "metadata": {},
   "source": [
    "### Task description\n",
    "Given an input sequence `XY[0-5]+` (two digits X and Y followed by a sequence of digits in the range from 0 to 5 inclusive), the task is to count the number of occurrences of X and Y in the remaining substring and then calculate the difference #X - #Y.\n",
    "\n",
    "Example:  \n",
    "Input: `1214211`  \n",
    "Output: `2`  \n",
    "Explanation: there are 3 `1`'s and 1 `2` in the sequence `14211`, `3-1=2`  \n",
    "  \n",
    "The model must learn this relationship between the symbols of the sequence and predict the output. This task can be solved with a multi-head attention network.\n",
    "\n",
    "$\\color{red}{\\textbf{Note}}$: In all your implementations, you're allowed to use only basic PyTorch operations. No APIs from external libraries such as Huggingface transformers should be used to solve any part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "widespread-pickup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28e5ad7f4f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 5\n",
    "VOCAB_SIZE = 6\n",
    "NUM_TRAINING_STEPS = 25000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-dietary",
   "metadata": {},
   "source": [
    "#### iv) Data generation function (0.5 points)\n",
    "Fill the code to calculate the ground truth outpu for the random sequence and store it in `gts`.    \n",
    "\n",
    "Why are we offseting the ground truth? In other words, why do we need grouth truth to be non-negative?\n",
    "\n",
    "$\\color{red}{\\text{Ans:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "based-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data samples as described at the beginning of the\n",
    "# script\n",
    "def get_data_sample(batch_size=1):\n",
    "    random_seq = torch.randint(low=0, high=VOCAB_SIZE - 1,\n",
    "                               size=[batch_size, SEQ_LEN + 2])\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO: Calculate the ground truth output for the random sequence and store\n",
    "    # it in 'gts'.\n",
    "    ############################################################################\n",
    "    gts = gts.squeeze()\n",
    "\n",
    "    # Ensure that GT is non-negative\n",
    "    ############################################################################\n",
    "    # TODO: Why is this needed?\n",
    "    ############################################################################\n",
    "    gts += SEQ_LEN\n",
    "    return random_seq, gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_sample(batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-blink",
   "metadata": {},
   "source": [
    "#### v) Scaled Dot-Product Attention (1 point)\n",
    "Implement a naive version of the Attention mechanism in the following class. Please do not deviate from the given structure. If you have ideas about how to optimize the implementation you can however note them in a comment or provide an additional implementation.  \n",
    "For implementation, refer to Section 3.2.1 and Figure 2 (left) in the paper. Keep the parameters to the forward pass trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excess-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # q, k, and v are batch-first\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-football",
   "metadata": {},
   "source": [
    "#### vi) Multi-Head Attention (1 point)\n",
    "Implement Multi-Head Attention mechanism on top of the Single-Head Attention mechanism in the following class. Please do not deviate from the given structure. If you have ideas about how to optimize the implementation you can however note them in a comment or provide an additional implementation.  \n",
    "For implementation, refer to Section 3.2.2 and Figure 2 (right) in the paper. Keep the parameters to the forward pass trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beginning-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_r = self.embed_dim // self.num_heads   # to evenly split q, k, and v across heads.\n",
    "        self.dotatt = Attention()\n",
    "\n",
    "        self.q_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.final_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        # xavier initialization for linear layer weights\n",
    "        nn.init.xavier_uniform_(self.q_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.final_linear_proj.weight)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # q, k, and v are batch-first\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Implement multi-head attention as described in Section 3.2.2\n",
    "        # of the paper.\n",
    "        ########################################################################\n",
    "        # shapes of q, k, v are [bsize, SEQ_LEN + 2, hidden_dim]\n",
    "        bsize = k.shape[0]\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-hacker",
   "metadata": {},
   "source": [
    "#### vii) Encoding Layer (1 point)\n",
    "Implement the Encoding Layer of the network.  \n",
    "Refer the following figure from the paper for the architecture of the Encoding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nuclear-beginning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/eAKQu.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.stack.imgur.com/eAKQu.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polyphonic-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embed_dim=num_hidden, num_heads=num_heads)\n",
    "        # TODO: add necessary member variables\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.att(x, x, x)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-maximum",
   "metadata": {},
   "source": [
    "#### viii) Network definition (2 points)\n",
    "Implement the forward pass of the complete network.\n",
    "The network must do the following:\n",
    "1. calculate embeddings of the input (with size equal to `num_hidden`)\n",
    "2. perform positional encoding\n",
    "3. perform forward pass of a single Encoding layer\n",
    "4. perform forward pass of a single Decoder layer\n",
    "5. apply fully connected layer on the output\n",
    "\n",
    "Because we are dealing with a simple task, the whole Decoder layer can be replaced with a single MultiHeadAttention block. Since our task is not sequence-to-sequence (Seq2Seq), but rather the classification of a sequence, the query (`Q` matrix) for the MultiHeadAttention block can be another learnable parameter (`nn.Parameter`) instead of processed output embedding.\n",
    "\n",
    "In the forward pass we must add a (trainable) positional encoding of our input embedding. Why is this needed? Can you think of another similar task where the positional encoding would not be necessary?\n",
    "\n",
    "$\\color{red}{\\text{Ans:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "narrow-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_encoding_layers=1, num_hidden=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        q = torch.empty([1, num_hidden])\n",
    "        nn.init.normal_(q)\n",
    "        self.q = nn.Parameter(q, requires_grad=True)\n",
    "        # TODO: implement\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-zoning",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Don't edit the following 2 cells. They must run without errors if you implemented the model correctly.  \n",
    "The model should converge to nearly 100% accuracy after ~4.5k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beginning-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network, loss function, and optimizer\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "for i in range(NUM_TRAINING_STEPS):\n",
    "    inputs, labels = get_data_sample(BATCH_SIZE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    accuracy = (torch.argmax(outputs, axis=-1) == labels).float().mean()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
    "              (i , NUM_TRAINING_STEPS - 1, loss.item(), accuracy.item()))\n",
    "    if i == NUM_TRAINING_STEPS - 1:\n",
    "        print('Final accuracy: %.3f, expected %.3f' %\n",
    "              (accuracy.item(), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-separation",
   "metadata": {},
   "source": [
    "#### ix) Analysis of the results (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-fundamental",
   "metadata": {},
   "source": [
    "Plot a graph with `num_steps` on the x-axis while keeping `loss` and `accuracy` on the y-axis.\n",
    "\n",
    "\n",
    "$\\color{red}{\\text{Ans:}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4c643-0157-4fd6-b9e2-f4d966f6f340",
   "metadata": {},
   "source": [
    "Briefly analyze the results you got. Does the model learn the underlying pattern in all the sequences? How can we improve the results / speed up the training process?\n",
    "\n",
    "$\\color{red}{\\text{Ans:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051d53a-5487-4c3d-99db-4985e2cda2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
