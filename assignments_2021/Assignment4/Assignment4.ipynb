{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNTI 21/22: Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** For all implementations in this assignment, make sure to use PyTorch whenever possible. Most computations on vectors and matrices can be implemented very efficiently using the PyTorch API. There is no need for looping over vectors etc using `for` loops. As a simple example, in order to compute the mean of a vector, just use `torch.mean()`. If you are not familiar with PyTorch please consult the PyTorch tutorials online. Further, in case of any doubts, the Piazza forum is the best place to ask questions and clarifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.3. Multiple Linear Regression (8.0 points)\n",
    "\n",
    "In this exercise you will learn about [*multiple linear regression*](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression) while also experimenting with hyperparameter tuning. Performing regression on one independent (or explanatory) variable and a scalar dependent variable is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called **multiple linear regression**. (Do not confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, you will be implementing a **multiple linear regression** model in Python/PyTorch using the [*vanilla Gradient Descent*](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) algorithm. Particularly, we will be using a variant of the **stochastic gradient descent** (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64, i.e. we go through the training samples, sampling 64 at a time, and perform gradient descent. Such a procedure is sometimes called as **mini-batch gradient descent** in the deep neural networks community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the training procedure has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. Here, we will set a *tolerance value* for the difference in error (i.e. change in Mean Squared Error (MSE) values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the latest parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations in order to find the best parameters (i.e. weights) for our model. For this so called *hyperparameter tuning* we will be using the validation data. \n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is, untouched throughout our experiments. Using the hyperparameter combination (for the least MSE) that we found above, we train the model *again* with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error). It is this *generalization error* that we want it to be as low as possible for some *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. . You can take a glance of the data using functions like *data.head()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "display(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# Get data as NumPy array\n",
    "data_np = data.values\n",
    "\n",
    "# Convert the data to Torch tensor\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3.2. Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix-vector format it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called [**design matrix**](https://en.wikipedia.org/wiki/Design_matrix#Definition), consists of data points in our dataset. Each row corresponds to a data point whereas each column represents a feature. Therefore, the dimension of $X$ is *(number of data points, number of features)*. $X$ can be also thought as of the vertical concatenation of data points of shape *(batch_size, num_features)*. To make things easier and computationally efficient, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions. <br > (Hint: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for a $1^{st}$ degree polynomial.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** (0.25 point) <br >\n",
    "Derive the gradient (w.r.t $\\textbf{w}$) for the regularized objective given in 4.3.2. \\[**Hint**: You can use your results from 3.2(a). However, make sure to adapt the result to the above objective\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "The answer is included in the pdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3. Matrix-vector format for higher order polynomial\n",
    "\n",
    "Written in matrix form, a linear regression model for second-order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring (i.e., Hadamard product of X with itself) of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "**Task 2:** (0.25 point) <br >\n",
    "Write down the matrix-vector format for an $8^{th}$ order linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "The answer is written in the pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3.4. Hyperparameters\n",
    "Next, we will experiment with three hyperparameters for developing our model:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that will achieve the lowest *test* error on our data). This approach is called **hyperparameter optimization or tuning**. For convenience and computational reasons, we will experiment with only three values for each of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix possible hyperparameters\n",
    "polynomial_orders = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-6, 1e-8]\n",
    "lambdas = [0.1, 0.5, 0.8]\n",
    "\n",
    "\n",
    "\n",
    "# Fix batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3.5. Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$.\n",
    "\n",
    "**Task 3:** (0.25 point) <br > \n",
    "Complete the following function which performs normalization (i.e. normalizes the columns of $X$). Use only PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "    min_c = data.min(0, keepdim=True)[0]\n",
    "    max_c = data.max(0, keepdim=True)[0]\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            data[i,j] = (data[i,j] - min_c[0][j])/(max_c[0][j] - min_c[0][j])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.6. Data Splitting and Shuffling\n",
    "Typically, we need to divide our data into 3 splits \\[ train (80%), validation (10%), and test (10%)\\] for experimentation purposes. And shuffle the training data during every epoch.\n",
    "\n",
    "**Task 4**: 0.25 points <br >\n",
    "Implement the following function `split_data()`. You can either implement it manually using `torch` or use sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "def split_data(data, n_train=3898, n_val=500, n_test=500):\n",
    "    # TODO: implement\n",
    "    X, y = data\n",
    "    X_train, temp_X_test, y_train, temp_y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(temp_X_test, temp_y_test, test_size=0.50, random_state=0)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    # shuffling of data along axis 0\n",
    "    X, y = shuffle(X_train, Y_train, random_state=0)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.7. Implementation of required functions\n",
    "\n",
    "**Task 5:** (0.5 point) <br >\n",
    "Complete the following function which computes the MSE value. You can ignore the regularization term and also the constant $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Mean Squared Error \n",
    "def compute_mse(prediction, ground_truth):\n",
    "    '''\n",
    "    :param prediction: a nx1 vector represents the prediciton of your model\n",
    "    :param ground_truth: a nx1 vector represents the ground_truth\n",
    "    :return: MSE loss\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    total_loss = 0\n",
    "    for i in range(len(prediction)):\n",
    "        diff = prediction[i] - ground_truth[i]\n",
    "        diff_s = diff * diff \n",
    "        total_loss = total_loss + diff_s\n",
    "    mse_loss = torch.sqrt(total_loss) / len(prediction)  \n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** (0.5 point) <br >\n",
    "Implement the function which computes the prediction of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(X, W):\n",
    "    '''\n",
    "    Given a design matrix X (could be a batch) and parameters W, calculate the prediction Yhat.\n",
    "    :param X: desgin matrix X of dimension nxk, where n is the number of data points (in the batch).\n",
    "    :param W: parameters\n",
    "    :return Yhat: the predictions\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    y_hat = torch.matmul(X, W)\n",
    "    \n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** (0.25 point) <br >\n",
    "Implement the function which computes the gradient of your loss function. That is, implement the gradient arrived at in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    '''\n",
    "    :param X: designmatrix X\n",
    "    :param Y: ground truth labels correspoinding to X\n",
    "    :param Yhat: predicted labels\n",
    "    :param W: parameters\n",
    "    :param lambda_: coefficient for the regularizer\n",
    "    :return: gradient w.r.t W\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    temp1 = torch.matmul(X.T, Yhat)\n",
    "    \n",
    "    temp2 = torch.matmul(X.T, Y.view(-1, 1))\n",
    "    \n",
    "    temp3 = lambda_ * W\n",
    "    \n",
    "    grad = temp1 - temp2 + temp3\n",
    "    \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8:** (0.5 point) <br >\n",
    "Implement the function which performs a single update step of mini-batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    '''\n",
    "    :param gradient: gradient at cur_W\n",
    "    :param lr: learning rate\n",
    "    :param cur_W: current value of parameters\n",
    "    :return new_W: perform parameter update (using gradient descent) and return new_W\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    new_W = cur_W - lr*gradient\n",
    "    \n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9:** (0.5 point) <br >\n",
    "Complete the following function which reformats your data as a design matrix, and accordingly the weight vector, for a given polynomial order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# concatenate matrix X horizontally, concatenate W vertically. That is, after this transformation, X gets broader and W gets longer\n",
    "# e.g. [1 X X^2 X^3], [1 W1 W2 W3]   #the square brackets does not signify python lists\n",
    "# \n",
    "def prepare_data_matrix(X, W, order):\n",
    "    # TODO: implement\n",
    "    ones = torch.ones((X.shape[0], 1))\n",
    "    one = torch.ones((1,1))\n",
    "    new_X = torch.cat((ones, X), 1)\n",
    "    new_W = torch.cat((one, W), 0)\n",
    "    if order > 1:\n",
    "        for o in range(1, order):\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(X.shape[1]):\n",
    "                    X[i, j] = X[i, j] * X[i, j]\n",
    "            new_X = torch.cat((new_X, X), 1)\n",
    "            new_W = torch.cat((new_W, W), 0)\n",
    "                \n",
    "    return new_X, new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.8. Training\n",
    "\n",
    "**Task 10:** (3 points) <br >\n",
    "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations.\n",
    "\n",
    "Note: You can also define a function, named appropriately (e.g. `train()`), which performs training. And, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_params = list()\n",
    "\n",
    "def epoch_train(new_X_train, Y_train, W_init, lr, l):\n",
    "    data_points = new_X_train.shape[0]\n",
    "    epochs = 10\n",
    "    mse_list = list()\n",
    "    for e in range(epochs):\n",
    "        total = 0\n",
    "        while(total < data_points):\n",
    "            if total+64 < data_points:\n",
    "                batch_x = new_X_train[total : total+64, :]\n",
    "                batch_y = Y_train[total : total+64]\n",
    "            else:\n",
    "                batch_x = new_X_train[total : data_points, :]\n",
    "                batch_y = Y_train[total : data_points]\n",
    "            yhat = get_prediction(batch_x, W_init)\n",
    "            grad = compute_gradient(batch_x, batch_y, yhat, W_init, l)\n",
    "            #print(grad.shape)\n",
    "            W_init = sgd(grad, lr, W_init)\n",
    "            total = total + 64\n",
    "        shuffle_train_data(new_X_train, Y_train)\n",
    "        p = get_prediction(new_X_train, W_init)\n",
    "        mse = compute_mse(p, Y_train)\n",
    "        mse_list.append(mse)\n",
    "    return W_init, mse_list\n",
    "\n",
    "def train(X_train, Y_train, W_init, polynomial_orders, learning_rates, lambdas):\n",
    "    # TODO: implement\n",
    "    for o in polynomial_orders:\n",
    "        for lr in learning_rates:\n",
    "            for l in lambdas:\n",
    "                params = list()\n",
    "                params.append(o)\n",
    "                params.append(lr)\n",
    "                params.append(l)\n",
    "                new_X_train, new_W = prepare_data_matrix(X_train, W_init, o)\n",
    "                l_W, mse_list = epoch_train(new_X_train, Y_train, new_W, lr, l)\n",
    "                params.append(l_W)\n",
    "                params.append(mse_list)\n",
    "                all_params.append(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $3$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11:** (0.25 point) <br >\n",
    "Complete the following function which selects the best hyperparameter combination given a set of weights (i.e. the one that gives lowest MSE on **validation data**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select hparams of minimum MSE on Validation data\n",
    "def select_best_hparams(X_val, W):\n",
    "    # TODO: Implement\n",
    "    pred = get_prediction(X_val, W) \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12:** (0.25 point) <br >\n",
    "Train the model for all possible hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination\n",
      "Weight :\n",
      "tensor([[1.8169],\n",
      "        [1.2055],\n",
      "        [0.8539],\n",
      "        [0.6208],\n",
      "        [0.9898],\n",
      "        [0.7288],\n",
      "        [0.8846],\n",
      "        [0.4155],\n",
      "        [0.4544],\n",
      "        [0.9230],\n",
      "        [0.5390],\n",
      "        [0.7993],\n",
      "        [1.0422],\n",
      "        [0.7394],\n",
      "        [0.4943],\n",
      "        [0.9307],\n",
      "        [0.6553],\n",
      "        [0.8034],\n",
      "        [0.2537],\n",
      "        [0.3665],\n",
      "        [0.7384],\n",
      "        [0.3793],\n",
      "        [0.6324],\n",
      "        [0.9777],\n",
      "        [0.7096],\n",
      "        [0.4614],\n",
      "        [0.9206],\n",
      "        [0.6449],\n",
      "        [0.7905],\n",
      "        [0.1876],\n",
      "        [0.3514],\n",
      "        [0.6226],\n",
      "        [0.3067],\n",
      "        [0.5207],\n",
      "        [0.9696],\n",
      "        [0.7071],\n",
      "        [0.4591],\n",
      "        [0.9202],\n",
      "        [0.6444],\n",
      "        [0.7901],\n",
      "        [0.1782],\n",
      "        [0.3509],\n",
      "        [0.5865],\n",
      "        [0.2909],\n",
      "        [0.4707],\n",
      "        [0.9695],\n",
      "        [0.7070],\n",
      "        [0.4591],\n",
      "        [0.9202],\n",
      "        [0.6444],\n",
      "        [0.7901],\n",
      "        [0.1779],\n",
      "        [0.3509],\n",
      "        [0.5814],\n",
      "        [0.2887],\n",
      "        [0.4560]], dtype=torch.float64)\n",
      "Polynomial :\n",
      "5\n",
      "Learnin Rate :\n",
      "1e-05\n",
      "Lambda :\n",
      "0.1\n",
      "Mse loss on training data :\n",
      "[tensor([0.0460], dtype=torch.float64), tensor([0.0431], dtype=torch.float64), tensor([0.0404], dtype=torch.float64), tensor([0.0379], dtype=torch.float64), tensor([0.0356], dtype=torch.float64), tensor([0.0336], dtype=torch.float64), tensor([0.0317], dtype=torch.float64), tensor([0.0299], dtype=torch.float64), tensor([0.0283], dtype=torch.float64), tensor([0.0269], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "# Train the model with all possible hyperparameter combinations\n",
    "normalized_X = data_normalization(data_tensor[:, 0:11])\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data((normalized_X , data_tensor[:, 11]))\n",
    "W = torch.rand((11,1), dtype=torch.float64)\n",
    "train(X_train, y_train, W,  polynomial_orders, learning_rates, lambdas)\n",
    "\n",
    "\n",
    "# Find best hyperparameter combination\n",
    "loss = 1000000\n",
    "b_o = -1\n",
    "b_lr = -1\n",
    "b_l = -1\n",
    "b_w = None\n",
    "b_m = None\n",
    "for i in range(len(all_params)):\n",
    "    params = all_params[i]\n",
    "    order = params[0]\n",
    "    lr = params[1]\n",
    "    lambda_ = params[2]\n",
    "    w = params[3]\n",
    "    m = params[4]\n",
    "    x_val, _ = prepare_data_matrix(X_val, w, order)\n",
    "    pred = select_best_hparams(x_val, w)\n",
    "    new_loss = compute_mse(pred, y_val)\n",
    "    if new_loss < loss :\n",
    "        b_w = w\n",
    "        b_o = order\n",
    "        b_lr = lr\n",
    "        b_l = lambda_\n",
    "        b_m = m\n",
    "        loss = new_loss\n",
    "\n",
    "print('Best combination')        \n",
    "print('Weight :')\n",
    "print(b_w)\n",
    "print('Polynomial :')\n",
    "print(b_o)\n",
    "print('Learnin Rate :')\n",
    "print(b_lr)\n",
    "print('Lambda :')\n",
    "print(b_l)\n",
    "print('Mse loss on training data :')\n",
    "print(b_m)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.9. Re-Training on Train + Validation data\n",
    "**Task 13:** (0.5 point) <br >\n",
    "Now, we will concatenate the training and validation data and make it as the new training data.\n",
    "Complete the following which does re-training on the combined training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the training on X_train + X_val combined\n",
    "# Train the model with all possible hyperparameter combinations\n",
    "new_X_train = torch.cat((X_train, X_val), 0)\n",
    "new_y_train = torch.cat((y_train, y_val), 0)\n",
    "train(X_train, y_train, W,  polynomial_orders, learning_rates, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Visualizing MSE over epochs:** \n",
    "\n",
    "**Task 14**: 0.25 points <br>\n",
    "Plot the MSE values (y-axis) against epochs (x-axis) using matplotlib.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fariaalam/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py:171: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "/Users/fariaalam/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbe3827e070>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the convergence of MSE values using matplotlib, i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "# TODO: implement\n",
    "fig, axes = plt.subplots()\n",
    "epochs = list(range(1,11))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse_loss')\n",
    "plt.title('epoch vs mse_loss plot with best hyperparameter for taining data')\n",
    "plt.plot(epochs, b_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.3.10. Evaluation on Test set\n",
    "**Task 15:** 0.25 points <br >\n",
    "Evaluate your model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1862706338019891\n"
     ]
    }
   ],
   "source": [
    "# test your model on X_test with the weight vector that you found above\n",
    "# this will be the generalization error of our model.\n",
    "x_test, _ = prepare_data_matrix(X_val, b_w, b_o)\n",
    "pred = get_prediction(x_test, b_w)\n",
    "test_mse = compute_mse(pred, y_val)\n",
    "print(test_mse.item())\n",
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.11. Results\n",
    "**Task 16:** 0.25 points <br >\n",
    "Report the MSE value on the test data. Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Used number of epochs for training: 10 \n",
    "MSE value on the test data : 0.1862706338019891\n",
    "polynomial order : 5\n",
    "learning rate : 1e-05\n",
    "lambda : 0.1\n",
    "\n",
    "We are given 3 ploynomial orders, among them 9 is very big and 1 is very small for 1 the parameters become underfitted and for ploynomial 9 the paramters become overfitted. For big learning rates the gradients oscilates too much and the model become unstable but for very small learning rate the model doesn't learn at all, thats why I think learning rate 1e-05 is giving better results. As regularizer co-efficient 0.1 is always better since it penalizes the model very efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 8.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
