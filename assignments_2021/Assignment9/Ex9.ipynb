{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3 Convolutional Neural Networks (6 points)\n",
    "## DO NOT EDIT THIS FILE\n",
    "You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed. In this exercise we are going to implement the following functions\n",
    "\n",
    "- Zero padding\n",
    "- Maxpool2D functions:\n",
    "    - Max pooling forward `__call__()`\n",
    "    - Pooling backward function `grad()`\n",
    "- Convolution2D functions:\n",
    "    - Convolution `__call__()`\n",
    "    - Convolution `grad()`\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear\n",
    "from layers import L2regularization, Dropout\n",
    "from layers import Conv2D, zero_padding\n",
    "from layers import Maxpool2D\n",
    "from model import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numpy_random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.a - Zero-Padding (1 point)\n",
    "\n",
    "\n",
    "Adding zero padding to images helps to avoid shrinking of the outputs as we build deeper networks\n",
    "\n",
    "Implement the `zero_padding()` function in layers/utils.py, which pads all the images of a batch of examples X with zeros. [Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). \n",
    "Note that the padding in an image is done on the borders of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(numpy_random_seed)\n",
    "\n",
    "# Create a dummy image batch\n",
    "image_batch = np.random.randn(5, 3, 3, 3)\n",
    "\n",
    "# Pad the batch using zero_padding function\n",
    "image_batch_padded = zero_padding(image_batch, 2)\n",
    "\n",
    "print (\"image_batch.shape =\", image_batch.shape)\n",
    "print (\"image_batch_padded.shape =\", image_batch_padded.shape)\n",
    "print (\"image_batch[1,1] =\", image_batch[1,1])\n",
    "print (\"image_batch_padded[1,1] =\", image_batch_padded[1,1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].set_title('image')\n",
    "ax[0].imshow(image_batch[0,:,:,0])\n",
    "ax[1].set_title('padded_image')\n",
    "ax[1].imshow(image_batch_padded[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3.b Max Pooling 2D forward pass (1 point)\n",
    "Pooling layers are used to shrink the height and width of the window. The max pooling operation slides a kernel window of $k \\times k$ and stores the maximum values in the window\n",
    "\n",
    "These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $k$. You take the maximum of the kernel window. Another pooling operation is global average pooling but we aren't going to cover it's implementations\n",
    "\n",
    "Now, you are going to implement the max pooling operation in the `Maxpool2D.py`. \n",
    "Implement the forward pass of the pooling layer in `__call__()`. Follow the hints in the documentation of max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(numpy_random_seed)\n",
    "X = np.random.randn(5, 5, 3, 2)\n",
    "maxpool2D = Maxpool2D()\n",
    "out = maxpool2D(X)\n",
    "print(\"out =\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.c Max Pooling 2D backward pass (2 points)\n",
    "Next, let's implement the backward pass for the pooling layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n",
    "\n",
    "Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called `create_mask()`  in MaxPool2D class which does the following: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "3 && 1 \\\\\n",
    "2 && 4\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "0 && 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This function creates a mask which keeps track of where the maximum is in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `grad()` function by following the instruction given in the file. You will use 4 for-loops (iterating over training examples, height, width, and channels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(numpy_random_seed)\n",
    "X_in_grad = np.random.randn(5, 4, 2, 2)\n",
    "print(np.mean(maxpool2D.grad(X_in_grad)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.d Convolutional Neural Networks - Forward pass (1 point)\n",
    "\n",
    "In the forward pass, you will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. You will then stack these outputs to get a 3D volume: \n",
    "\n",
    "\n",
    "Implement the `__call__()` function in `layers/Conv2D.py` to convolve the filters W on an tensor X. This class takes as input the number of channels of the input, the number of channels of the output, stride, kernel size and padding\n",
    "\n",
    "Follow the instructions in the file `layers/Conv2D.py` to implement the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(numpy_random_seed)\n",
    "conv_layer = Conv2D(in_channels = 3, out_channels=8, padding=1, stride=1)\n",
    "np.random.seed(numpy_random_seed)\n",
    "X = np.random.randn(14, 32, 32,3)\n",
    "\n",
    "Z = conv_layer(X)\n",
    "print(\"Mean Z = \", np.mean(Z))\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.e Conv backward pass (2 points)\n",
    "\n",
    "In previous exercises we computed the gradient for most of the layers that can be used to update the parameters. But in this exercise we aren't going to perform the update step because convolutions take too long to train on CPUs. Hence we keep this exercise only to the point of calculating the gradients without updating the model parameters. Nevertheless it's instructive to know the math behind the backpropagation of CNNs. The grad function is supposed to return three parameters\n",
    "- Gradient wrt to input of the layer\n",
    "- Gradient wrt to weights of the layer\n",
    "- Gradient wrt to biases of the layer\n",
    "Let's start by implementing the backward pass for a CONV layer. In the following description, $dL/dZ$ corresponds to the in_gradient of the layer that you receive as an argument. With a little abuse of notation we write it as $dLZ$.\n",
    "\n",
    "#### Computing dL/dX:\n",
    "This is the formula for computing $dL/dX$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$ (dL/dX) += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dLZ_{h,w}$$\n",
    "\n",
    "#### Computing dL/dW:\n",
    "We calculate the derivative of one filter with respect to loss by\n",
    "\n",
    "$$ (dL/dW)_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} X_{window} \\times dLZ_{h,w} $$\n",
    "\n",
    "$X_{window}$ corresponds to the slice of the input which was used to generate the acitivation $Z_{ij}$. \n",
    "\n",
    "#### Computing dL/db:\n",
    "\n",
    "For computing $dL/db$ with respect to the loss for a certain filter $W_c$. $dL/db$ is computed by summing $dLZ$ for the particular channel. In this case, you are just summing over all the in gradients with respect to the loss. \n",
    "\n",
    "$$ (dL/db) = \\sum_h \\sum_w dLZ_{h,w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(numpy_random_seed)\n",
    "print(Z.shape)\n",
    "dLdx, dLdW, dLdb = conv_layer.grad(Z)\n",
    "print(\"dX mean =\", np.mean(dLdx))\n",
    "print(\"dW mean =\", np.mean(dLdW))\n",
    "print(\"db mean =\", np.mean(dLdb))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3137699d1f075e053afc7010c914745d752362d49097efda5036a6d434f0174"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
