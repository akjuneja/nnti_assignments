{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjzINBTuCOMI"
      },
      "source": [
        "# Exercise 5.4 (4 points)\n",
        "### **Do not edit this notebook**\n",
        "Implement the __call__() method in every activation function. You don't have to implement backward() anywhere for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w0c1zfdFCOMK"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8AL3CU5COMM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
        "from losses import CrossEntropy, MSELoss\n",
        "from layers import Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6US681OACOMN"
      },
      "source": [
        "## Ex5.4 a Implement ReLU() activation function (0.5 point)\n",
        "Implement the __call__ function in ./activations/ReLU.py which takes a tensor x as input and applies the ReLU activation function on x. \n",
        "\n",
        "f(x) = max(0,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke3fLFs9COMO",
        "outputId": "8848eae3-95fe-45ce-ce2b-356072804f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1 0.  0.5 0.9 0.  0. ]\n"
          ]
        }
      ],
      "source": [
        "relu = ReLU()\n",
        "x = np.array([0.1, -0.3, 0.5, 0.9, 0, -1.0])\n",
        "x_relu = relu(x)\n",
        "print(x_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAw1ph3GCOMP"
      },
      "source": [
        "## Ex5.4 b Implement LeakyReLU() activation function (0.5 point)\n",
        "Implement the __call__ function in ./activations/LeakyReLU.py which takes a tensor x as input and applies the LeakyReLU activation function on x.\n",
        "\n",
        "$f(x) = \\alpha x \\text{ if } x < 0$    \n",
        "$f(x) =   x \\text{ if } x \\geq 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uD_dYSACOMQ",
        "outputId": "339e285e-46de-41db-a5d1-daf376e48d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.1   -0.015  0.5    0.9    0.    -0.05 ]\n"
          ]
        }
      ],
      "source": [
        "leaky_relu = LeakyReLU(alpha=0.01)\n",
        "x = np.array([0.1, -0.3, 0.5, 0.9, 0, -1.0])\n",
        "x_lekyrelu = leaky_relu(x)\n",
        "print(x_lekyrelu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVWApjOfCOMR"
      },
      "source": [
        "## Ex5.4 c Implement Tanh() activation function (0.5 point)\n",
        "Implement the __call__ function in ./activations/Tanh.py which takes a tensor x as input and applies the Softmax activation function on x.  \n",
        "$f(x) = \\tanh(x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3aSE-JJCOMS",
        "outputId": "a307a3d8-9fc0-4240-da46-170bee1d4644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.09966799 -0.29131261  0.46211716  0.71629787  0.         -0.76159416]\n"
          ]
        }
      ],
      "source": [
        "tanh = Tanh()\n",
        "x_tanh = tanh(x)\n",
        "print(x_tanh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvVES45UCOMT"
      },
      "source": [
        "## Ex5.4 d Implement Softmax() activation function (0.5 point)\n",
        "Implement the __call__ function in ./activations/Softmax.py which takes a tensor x as input and applies the Softmax activation function on x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl90MjHiCOMU",
        "outputId": "68eb0a40-16cf-4cca-f000-72b8e374922d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.15093442 0.10117436 0.22516769 0.33591072 0.13657111 0.0502417 ]\n"
          ]
        }
      ],
      "source": [
        "softmax = Softmax()\n",
        "x_softmax = softmax(x)\n",
        "print(x_softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWJ36u-SCOMV"
      },
      "source": [
        "## Ex5.4 e Implement Sigmoid() activation function (0.5 point)\n",
        "Implement the __call__ function in ./activations/Sigmoid.py which takes a tensor x as input and applies the Sigmoid activation function on x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6h1bjVMCOMV",
        "outputId": "45b8d700-14df-4ad2-db04-d9ae6868cc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.52497919 0.42555748 0.62245933 0.7109495  0.5        0.26894142]\n"
          ]
        }
      ],
      "source": [
        "sigmoid = Sigmoid()\n",
        "x_sigmoid = sigmoid(x)\n",
        "print(x_sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiE97Iz0COMW"
      },
      "source": [
        "## Ex5.4 f Implement CrossEntropy Loss (0.5 point)\n",
        "mplement the __call__ function in ./losses/CrossEntropy.py which takes a predictions and true values as agruments and finds the cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-umSNsi3COMX",
        "outputId": "afab9ab6-7890-4af4-e4af-543d327098ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.941608539858445\n"
          ]
        }
      ],
      "source": [
        "ce_loss = CrossEntropy()\n",
        "predictions = np.array([[0.4,0.35,0.71,0.60],\n",
        "                        [0.01,0.01,0.01,0.65]])\n",
        "targets = np.array([[0,0,0,1],\n",
        "                  [0,0,0,1]])\n",
        "ce_loss_num = ce_loss(predictions, targets)\n",
        "print(ce_loss_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDwKsmxXCOMY"
      },
      "source": [
        "## Ex5.4 g Implement MSE Loss (0.5 point)\n",
        "Implement the __call__ function in ./losses/MSELoss.py which takes a predictions and true values as agruments and finds the mean squared error loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZCee7O1COMZ",
        "outputId": "29b2bb19-3e39-4551-d050-347fdfdb6791"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.49000000000000005"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mse_loss = MSELoss()\n",
        "y = np.array([0.6, 0.3, 0.5, 0.1, 1.3, -1.0])\n",
        "mse_loss(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ermYOZ30COMa"
      },
      "source": [
        "## Ex5.4 h Implement Linear() layer (0.5 point)\n",
        "Implement the __call__ function in ./layers/Linear.py which is y = Wx + b where b is the bias variable. We will do a simple forward pass of with two linear layers and one activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLaETFjwCOMb",
        "outputId": "886276b0-a4de-407a-c504-4eb819c7097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensions for h1  (20, 20)\n",
            "Dimensions for z1  (20, 20)\n",
            "Dimensions for h2  (20, 10)\n",
            "Dimensions for z2  (20, 10)\n"
          ]
        }
      ],
      "source": [
        "input_data = np.random.randn(20, 100)\n",
        "layer1 = Linear(100, 20)\n",
        "layer2 = Linear(20, 10)\n",
        "\n",
        "h1 = layer1(input_data)\n",
        "print(\"Dimensions for h1 \",h1.shape)\n",
        "z1 = relu(h1)\n",
        "print(\"Dimensions for z1 \",z1.shape)\n",
        "h2 = layer2(z1)\n",
        "print(\"Dimensions for h2 \",h2.shape)\n",
        "z2 = softmax(h2)\n",
        "print(\"Dimensions for z2 \",z2.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ex5_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
