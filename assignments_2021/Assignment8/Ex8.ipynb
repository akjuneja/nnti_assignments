{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import ReLU, LeakyReLU, Tanh, Softmax, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear\n",
    "from layers import L2regularization, Dropout\n",
    "from model import Model\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to strictly follow the cell structure, but please keep the overall organisation of the notebook the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Stochastic Gradient Descent (1.5 points)\n",
    "In this exercise you are given the PyTorch model and the training loop which uses Stochastic Gradient Descent. We train the model for 10 epochs with batch size equal to 4.   \n",
    "Your task is to implement the same model using Model class which we got from the previous assignment. Use the same hyperparameters (batch size, number of epochs, learning rate) for training. Adapt the training process from the previous assignment so that it uses mini-batches instead of the loading the whole training set at once. You are expected to achieve the same performance on your model as with PyTorch model (around 80% accuracy on the test data after training for 10 epochs).   \n",
    "Additionally, record both the training and test loss every 2000 minibatches both for PyTorch model and your model. Plot the loss graphs and comment on the differences between them (if any). For each model, the graphs of train and test loss should be displayed on one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),])\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 200, bias=False)\n",
    "        self.layer2 = nn.Linear(200, 80, bias=False)\n",
    "        self.layer3 = nn.Linear(80, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = torch.sigmoid(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "np.random.seed(123)\n",
    "\n",
    "# Your code for defining a model goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 10 epochs using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model by computing the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss graphs for PyTorch model and your model (train and test losses should be on the same plot) \n",
    "# and comment on differences (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Stochastic Gradient Descent with Momentum (1.5 points)\n",
    "As it was discussed during the lecture, momentum helps to accelerate gradient in the right direction and helps to solve some problems related with optimization.  \n",
    "Train the PyTorch model using SGD with Momentum while keeping other hyperparameters the same. Try to find the optimal value for momentum for the given problem (you can use the test data as validation data since it's more or less toy exercise).  \n",
    "Implement the training with momentum for your Model class. Your code must be contained in `sgd_momentum` method in model.py. You can change the arguments of the function according to your needs, but please keep the implementation there. \n",
    "Evaluate the performance of both models on test data. Did the accuracy improved for the same number of epochs trained?  \n",
    "Again, keep the record of train loss and test loss every 2000 minibatches for both models, plot them and comment on the differences between two models and between SGD with Momentum and without Momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate momentum for training the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate momentum for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss graphs for PyTorch model and your model (train and test losses should be on the same plot) \n",
    "# and comment on differences (if any) between two models and between SGD with Momentum and without Momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3 AdaGrad (1.5 points)\n",
    "Train the PyTorch model using AdaGram while keeping other hyperparameters the same  \n",
    "Implement the training with AdaGrad for your Model class. Your code must be contained in to `ada_grad` method in model.py. You can change the arguments of the function according to your needs, but please keep the implementation there. \n",
    "Evaluate the performance of both models on test data. Did the accuracy improved for the same number of epochs trained?  \n",
    "Again, keep the record of train loss and test loss after every 2000 minibatches for both models, plot them and comment on the differences between two models and between AdaGrad and the previous optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate AdaGrad for training the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate AdaGrad for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss graphs for PyTorch model and your model (train and test losses should be on the same plot) \n",
    "# and comment on differences (if any) between two models and between AdaGrad and the previous optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.4 Adam (1.5 points)\n",
    "Train the PyTorch model using Adam while keeping other hyperparameters the same  \n",
    "Implement the training with Adam for your Model class. Your code must be contained in to `adam` method in model.py. You can change the arguments of the function according to your needs, but please keep the implementation there. \n",
    "Evaluate the performance of both models on test data. Did the accuracy improved for the same number of epochs trained?\n",
    "Again, keep the record of train loss and test loss every 2000 minibatches for both models, plot them and comment on the differences between two models and between Adam and the previous optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate Adam for training the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate Adam for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss graphs for PyTorch model and your model (train and test losses should be on the same plot) \n",
    "# and comment on differences (if any) between two models and between Adam and the previous optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
